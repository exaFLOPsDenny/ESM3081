## Unsupervised Learning: Clustering

### Hierarchical Cluster
> Unlike k-means cluster, fixed # of clusters, Hierarchical clustering  produces a set of nested organized as a hierachical tree, which can be visualized as a dendrogram.

<img width="500" alt="image" src="https://github.com/user-attachments/assets/c537f986-5737-4ff2-9504-005442c55009">

- Strengths
    - Good Visual representation, correspond to meaningful taxonomies
    - No assumption on # of clusters
- Weeknesses
    - Once a decision is made to combine two clusters, it cannot be undone
    - Computationally expensive
    - Highly depends on scaling of features
    - Fails at separating complex shapes

- Agglomerative clustering
    - Start with the data points as **individual clusters**  
    - At each step, merge the closest pair of clusters until only one cluster(or k clusters) left  
    <img width="500" alt="image" src="https://github.com/user-attachments/assets/7e837775-3dd8-472d-ad82-363a8671d659">  

    - Dendrogram helps to determine the number of clusters
    <img width="500" alt="image" src="https://github.com/user-attachments/assets/0abd7065-1a21-4766-8635-40bb24e5a50c">  
    - No predict method 


- Divisive clustering
    - Start with **one** all-inclusive cluster
    - At each step, split a cluster until each cluster contains an individual point(or k clusters)  

#### Linkage criteria

- Single Linkage
    - **Minimum** distance between all data points
    - Can handle non-globular shapes
    - Sensitive to noise and outliers
    <img width="300" alt="image" src="https://github.com/user-attachments/assets/efc3f464-4857-4ed9-98aa-7f03b29f6d7e">

- Complete Linkage
    - **Maximum** distance between all data points
    - Less susceptible to noise and outliers
    - Tend to break large clusters
    - Biased towards golbular clusters
    <img width="300" alt="image" src="https://github.com/user-attachments/assets/490a22c6-c8b0-45f6-b78d-5eed2959a1e4">

- Average Linkage
    - **Average** distance between all data points
    - Compromise between Single and Complete linkage
    - Less susceptible to noise and outliers
    - Biased towards globular clusters
    <img width="300" alt="image" src="https://github.com/user-attachments/assets/2dbefee9-93aa-466b-9189-2e2f518d595b">

- Ward Linkage
    - **Minimum increase in variance when two clusters are merged**
    - Less susceptible to noise and outliers
    - Often leads to clusters that are relatively equally sized
    - Hierarchical analogue of K-means

#### Hyperparmeters

- Affinity(distance metric)
    - Like k-means clustering, Hierachical clustering will be affected by the distance of data. Euclidead is default, and feature scaling, one-hot encoding might be needed
- linkage
- n_clusters or distance_threshold

