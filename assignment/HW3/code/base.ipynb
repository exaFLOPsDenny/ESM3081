{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["import torch\n", "import torch.nn as nn\n", "import torch.nn.functional as F\n", "import torch.optim as optim\n", "from torch.utils.data import DataLoader, TensorDataset\n", "from sklearn.preprocessing import StandardScaler\n", "from sklearn.model_selection import train_test_split\n", "from sklearn.preprocessing import LabelEncoder\n", "import numpy as np\n", "import pandas as pd\n", "from ucimlrepo import fetch_ucirepo  # Import the dataset fetching function\n", "import matplotlib.pyplot as plt\n", "import seaborn as sns"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. Fetch Dry Bean Dataset using ucimlrepo"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["dry_bean = fetch_ucirepo(id=602)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Data (features and targets as pandas dataframes)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = dry_bean.data.features\n", "y = dry_bean.data.targets"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Handle missing values by dropping rows with missing values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X = X.dropna()\n", "y = y.loc[X.index]  # Ensure targets match the remaining data points"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Encode class labels as integers"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["le = LabelEncoder()\n", "y = le.fit_transform(y)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Split data into training, validation, and test sets (60% train, 20% val, 20% test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42)  # 60% train\n", "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)  # 20% val, 20% test"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Standardize the features"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["scaler = StandardScaler()\n", "X_train = scaler.fit_transform(X_train)\n", "X_val = scaler.transform(X_val)\n", "X_test = scaler.transform(X_test)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Convert the data to PyTorch tensors"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["X_train = torch.tensor(X_train, dtype=torch.float32)\n", "X_val = torch.tensor(X_val, dtype=torch.float32)\n", "X_test = torch.tensor(X_test, dtype=torch.float32)\n", "y_train = torch.tensor(y_train, dtype=torch.long)\n", "y_val = torch.tensor(y_val, dtype=torch.long)\n", "y_test = torch.tensor(y_test, dtype=torch.long)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Create DataLoader for PyTorch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["batch_size = 32\n", "train_dataset = TensorDataset(X_train, y_train)\n", "val_dataset = TensorDataset(X_val, y_val)\n", "test_dataset = TensorDataset(X_test, y_test)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n", "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n", "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2. Define Neural Network Model in PyTorch"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["class NeuralNet(nn.Module):\n", "    def __init__(self, input_size, num_classes):\n", "        super(NeuralNet, self).__init__()\n", "        self.fc1 = nn.Linear(input_size, 64)\n", "        self.dropout = nn.Dropout(0.5)  # Dropout layer for MC Dropout\n", "        self.fc2 = nn.Linear(64, 64)\n", "        self.fc3 = nn.Linear(64, num_classes)\n", "    \n", "    def forward(self, x):\n", "        x = torch.relu(self.fc1(x))\n", "        x = self.dropout(x)\n", "        x = torch.relu(self.fc2(x))\n", "        x = self.fc3(x)\n", "        return x"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Initialize the model, loss function, and optimizer"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["input_size = X_train.shape[1]\n", "num_classes = len(np.unique(y_train))\n", "model = NeuralNet(input_size, num_classes)\n", "criterion = nn.CrossEntropyLoss()\n", "optimizer = optim.Adam(model.parameters(), lr=0.001)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. Training Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def train(model, train_loader, criterion, optimizer, epochs=50):\n", "    model.train()\n", "    for epoch in range(epochs):\n", "        running_loss = 0.0\n", "        for i, (inputs, labels) in enumerate(train_loader):\n", "            optimizer.zero_grad()\n", "            outputs = model(inputs)\n", "            loss = criterion(outputs, labels)\n", "            loss.backward()\n", "            optimizer.step()\n", "            running_loss += loss.item()\n", "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["4. Evaluation Function"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def evaluate(model, test_loader):\n", "    model.eval()\n", "    correct = 0\n", "    total = 0\n", "    with torch.no_grad():\n", "        for inputs, labels in test_loader:\n", "            outputs = model(inputs)\n", "            _, predicted = torch.max(outputs, 1)\n", "            total += labels.size(0)\n", "            correct += (predicted == labels).sum().item()\n", "    accuracy = 100 * correct / total\n", "    return accuracy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Train the model"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["train(model, train_loader, criterion, optimizer, epochs=512)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Evaluate without reject option"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy = evaluate(model, test_loader)\n", "print(f\"Accuracy without reject option: {accuracy:.4f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["5. Uncertainty Quantification"]}, {"cell_type": "markdown", "metadata": {}, "source": ["1. Confidence Uncertainty Measure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def confidence_uncertainty(softmax_probs):\n", "    max_probs, _ = torch.max(softmax_probs, dim=1)\n", "    return -max_probs"]}, {"cell_type": "markdown", "metadata": {}, "source": ["2. Margin Uncertainty Measure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def margin_uncertainty(softmax_probs):\n", "    sorted_probs, _ = torch.sort(softmax_probs, dim=1, descending=True)\n", "    margin = sorted_probs[:, 0] - sorted_probs[:, 1]\n", "    return -margin"]}, {"cell_type": "markdown", "metadata": {}, "source": ["3. Entropy Uncertainty Measure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def entropy_uncertainty(softmax_probs):\n", "    entropy = -torch.sum(softmax_probs * torch.log(softmax_probs + 1e-10), dim=1)  # Add small epsilon for numerical stability\n", "    return entropy"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Get predictions and calculate uncertainties"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def calculate_uncertainties(model, test_loader):\n", "    model.eval()\n", "    confidence_list = []\n", "    margin_list = []\n", "    entropy_list = []\n", "    \n", "    with torch.no_grad():\n", "        for inputs, _ in test_loader:\n", "            outputs = model(inputs)\n", "            softmax_probs = F.softmax(outputs, dim=1)\n", "            \n", "            # Calculate uncertainties\n", "            confidence_list.append(confidence_uncertainty(softmax_probs))\n", "            margin_list.append(margin_uncertainty(softmax_probs))\n", "            entropy_list.append(entropy_uncertainty(softmax_probs))\n", "    \n", "    # Convert lists to tensors\n", "    confidence_uncertainty_tensor = torch.cat(confidence_list)\n", "    margin_uncertainty_tensor = torch.cat(margin_list)\n", "    entropy_uncertainty_tensor = torch.cat(entropy_list)\n", "    \n", "    return confidence_uncertainty_tensor, margin_uncertainty_tensor, entropy_uncertainty_tensor"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run uncertainty calculations"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["confidence_uncertainty_vals, margin_uncertainty_vals, entropy_uncertainty_vals = calculate_uncertainties(model, test_loader)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["For example, let's print the first few uncertainty values"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(\"Confidence Uncertainty (first 5):\", confidence_uncertainty_vals[:5])\n", "print(\"Margin Uncertainty (first 5):\", margin_uncertainty_vals[:5])\n", "print(\"Entropy Uncertainty (first 5):\", entropy_uncertainty_vals[:5])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Function to test different thresholds for a given uncertainty measure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def optimize_threshold(uncertainty_vals, y_true, model, val_loader, num_thresholds=100):\n", "    # Create thresholds between the min and max of uncertainty values\n", "    range_min = uncertainty_vals.min().item()\n", "    range_max = uncertainty_vals.max().item()\n", "    \n", "    thresholds = torch.linspace(range_min, range_max, num_thresholds)\n", "    \n", "    best_threshold = None\n", "    best_accuracy = 0.0\n", "    best_rejection_rate = 0.0\n", "    for threshold in thresholds:\n", "        accepted_idx = uncertainty_vals <= threshold  # Accept if uncertainty is below threshold\n", "        rejected_idx = uncertainty_vals > threshold   # Reject if uncertainty is above threshold\n", "        \n", "        # Calculate accuracy for accepted samples\n", "        accepted_samples = torch.nonzero(accepted_idx, as_tuple=True)[0]\n", "        y_accepted = y_true[accepted_samples]\n", "        if len(accepted_samples) > 0:\n", "            correct = 0\n", "            total = 0\n", "            for i, (inputs, labels) in enumerate(val_loader):\n", "                if i in accepted_samples:\n", "                    outputs = model(inputs)\n", "                    _, predicted = torch.max(outputs, 1)\n", "                    total += labels.size(0)\n", "                    correct += (predicted == labels).sum().item()\n", "            accuracy = 100 * correct / total if total > 0 else 0.0\n", "        else:\n", "            accuracy = 0.0\n", "        rejection_rate = 100 * (1 - len(accepted_samples) / len(uncertainty_vals))\n\n", "        # Choose the best threshold based on accuracy\n", "        if accuracy > best_accuracy:\n", "            best_accuracy = accuracy\n", "            best_threshold = threshold\n", "            best_rejection_rate = rejection_rate\n", "    return best_threshold, best_accuracy, best_rejection_rate"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Apply the optimization for each uncertainty measure"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def find_best_thresholds(model, val_loader, y_val):\n", "    confidence_uncertainty_vals, margin_uncertainty_vals, entropy_uncertainty_vals = calculate_uncertainties(model, val_loader)\n", "    \n", "    print(\"Optimizing thresholds for uncertainty measures...\")\n", "    \n", "    # Optimize thresholds for each uncertainty measure\n", "    best_conf_threshold, best_conf_accuracy, best_conf_rejection_rate = optimize_threshold(confidence_uncertainty_vals, y_val, model, val_loader)\n", "    best_margin_threshold, best_margin_accuracy, best_margin_rejection_rate = optimize_threshold(margin_uncertainty_vals, y_val, model, val_loader)\n", "    best_entropy_threshold, best_entropy_accuracy, best_entropy_rejection_rate = optimize_threshold(entropy_uncertainty_vals, y_val, model, val_loader)\n", "    print(f\"Best Confidence Threshold: {best_conf_threshold:.4f}, Accuracy: {best_conf_accuracy:.4f}, Rejection Rate: {best_conf_rejection_rate:.2f}%\")\n", "    print(f\"Best Margin Threshold: {best_margin_threshold:.4f}, Accuracy: {best_margin_accuracy:.4f}, Rejection Rate: {best_margin_rejection_rate:.2f}%\")\n", "    print(f\"Best Entropy Threshold: {best_entropy_threshold:.4f}, Accuracy: {best_entropy_accuracy:.4f}, Rejection Rate: {best_entropy_rejection_rate:.2f}%\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Run the threshold optimization on the validation set"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["find_best_thresholds(model, val_loader, y_val)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.6.4"}}, "nbformat": 4, "nbformat_minor": 2}